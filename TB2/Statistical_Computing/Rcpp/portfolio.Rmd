---
title: "Rcpp Portfolio"
author: "Rachel Wood"
date: "2023-03-27"
output:
  pdf_document: 
    highlight: tango
  html_document:
    df_print: paged
header-includes: 
  - \usepackage{tikz}
  - \usepackage{fvextra}
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(ggplot2)
theme_set(theme_bw())
scale_colour_discrete <- function(){
  scale_colour_manual(values = c("mediumorchid3", "darkseagreen3", "lightpink2"))
}

knitr::opts_knit$set(dev = 'pdf',
              pdf.options(encoding = "ISOLatin9.enc"),
              message = FALSE,
              warning = FALSE,
              tidy.opts = list(width.cutoff=50), tidy=TRUE,
              fig.pos ="H", out.width = '60%',
              fig.align = 'center'
              )

```

For this portfolio, we use Rcpp to fit an adaptive kernel smoothing regression model. 

We first generate data according to the model
$$
y_i = \sin (\alpha \pi x^3) + z_i \hspace{0.2cm} \text{ with } \hspace{0.2cm} z_i \sim \mathcal{N} (0, \sigma^2)
$$

In this case we take $\alpha = 4$ and $\sigma = 0.2$.

```{r}
library(dplyr)
library(ggplot2)
n <- 400 
alpha <- 4
sigma <- 0.2

x <- runif(n)
y <- sin(alpha * pi * x^3) + rnorm(n, sd = sigma)

data <- tibble(x = x, y =y)

ggplot(data = data, aes(x, y))+
  geom_point(size = 0.8)
```

# The Kernel Smoother

We model $\mu(x) = \mathbb{E}  (y|x)$ by
$$
\hat \mu (x) = \frac{\sum_{i=1}^n \kappa_\lambda(x,x_i)y_i}{\sum_{i=1}^n \kappa_\lambda(x,x_i)}
$$
where we take $\kappa_\lambda$ to be a Gaussian kernel with variance $\lambda^2$.

We implement this with the following function:

```{r}
meanKRS <- function(x, y, xnew, lambda){
  n <- length(x)
  nnew <- length(xnew)
  
  mu <- numeric(nnew)
  
  for (i in 1:nnew){
    mu[i] <- sum(dnorm(x,xnew[i], lambda)*y)/ sum(dnorm(x,xnew[i], lambda))
  }
  
  return(mu)
}
```

We can now compare the fits for different values of $\lambda$:
```{r}
library(tidyr)
xnew <- seq(0,1, length.out = 1000)

smooth_large <- meanKRS(x, y, xnew, lambda = 0.06)
smooth_medium <- meanKRS(x, y, xnew, lambda = 0.04)
smooth_small <- meanKRS(x, y, xnew, lambda = 0.02)

plot_data <- tibble(x = xnew) %>%
  mutate("0.06" = smooth_large,
         "0.04" = smooth_medium,
         "0.02" = smooth_small) %>%
  pivot_longer(cols = c("0.06","0.04","0.02"),
               names_to = "lambda",
               values_to = "fitted") %>%
  mutate(lambda = as.factor(lambda)) 

ggplot() +
  geom_point(data = data,
             aes(x, y), size = 0.8) +
  geom_line(data = plot_data, 
            aes(x, fitted, color = lambda), linewidth = 0.7)
```
We now use Rcpp to write a C++ version of `meanKRS()`:
```{r}
library(Rcpp)

```

```{r, engine='Rcpp'}
#include <Rcpp.h>
#include <Rmath.h>
using namespace Rcpp;

// [[Rcpp::export]]

NumericVector meanKRS_Rcpp(const NumericVector x, const NumericVector y, const NumericVector xnew, const double lambda) {
  int n = x.size();
  int nnew = xnew.size();
  
  NumericVector mu(nnew);
  
  for (int i = 0; i < nnew; i++){
    mu[i] = sum(dnorm(x,xnew[i], lambda)*y)/ sum(dnorm(x,xnew[i], lambda));
  }
  
  return mu;
}
```

We check that this function produces the same output as the R version, 

```{r}
max(meanKRS(x, y, xnew, lambda = 0.06) - meanKRS_Rcpp(x, y, xnew, lambda = 0.06))
```
and compare the performance of the two functions using the `microbenchmark()` function:
```{r}
library(microbenchmark)
microbenchmark("R" = meanKRS(x, y, xnew, lambda = 0.06),
               "Rcpp" = meanKRS_Rcpp(x, y, xnew, lambda = 0.06))
```

# Cross-Validation

We now implement a cross-validation procedure for finding the optimal $\lambda$, using the mean squared error of the test set as the metric for determining the fit of our model. We first write the R version of this function:
```{r}
mse_lambda <- function(log_lambda, x, y, x_new, y_new){
  lambda <- exp(log_lambda)
  
  fitted <- meanKRS(x, y, x_new, lambda)
  return(sum((fitted - y_new)^2))
}

lambda_cv <- function(x, y, groups){
  n <- length(x)
  
  
  lambdas <- numeric(nfolds)
  mse <- numeric(nfolds)
  
  for (i in 1:nfolds){
    x_train <- x[groups != i]
    y_train <- y[groups != i]
    
    x_test <- x[groups == i]
    y_test <- y[groups == i]
    
    solution <- optim(par = 0.02, fn = mse_lambda, x = x_train, y = y_train, x_new = x_test, y_new =  y_test, method = "BFGS")
    lambdas[i] <- exp(solution$par)
    mse[i] <- solution$value
    
  }
  
 min_ind <- which.min(mse)
 return(lambdas[min_ind])
}
```

We now plot the smooth for the returned value $\lambda$ to see if this seems reasonable:
```{r}
nfolds <- 5
groups <- sample(rep(1:nfolds, length.out = n), size = n)
hat_lambda <- lambda_cv(x, y, groups)
opt_smooth <- meanKRS(x, y, xnew, hat_lambda)
opt_data <- tibble(xnew = xnew,
         fitted = opt_smooth)
ggplot() +
  geom_point(data = data, aes(x, y), size = 0.8) +
  geom_line(data = opt_data, aes(x = xnew,y = fitted), 
            color = "mediumorchid3", linewidth = 0.7)
```

We now write the equivalent function in Rcpp using the `roptim` package:

```{r, engine='Rcpp'}
// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::depends(roptim)]]

#include <cmath>  
#include <cstddef>

#include <algorithm>

#include <RcppArmadilloExtensions/sample.h>
#include <RcppArmadillo.h>
#include <roptim.h>
#include <functional>
using namespace Rcpp;
using namespace arma;
using namespace roptim;


double mse_lambda(const double lambda, const NumericVector x,const NumericVector y,const NumericVector xnew, const NumericVector y_new){
  int n = x.size();
  int nnew = xnew.size();
  
  NumericVector fitted(nnew);
  
  for (int i = 0; i < nnew; i++){
    fitted[i] = sum(dnorm(x,xnew[i], lambda)*y)/ sum(dnorm(x,xnew[i], lambda));
  }
  return sum(pow(fitted - y_new, 2));
}


NumericVector x_train, y_train, y_test, x_test;


// [[Rcpp::export]]


double lambda_cv_Rcpp(const NumericVector x, const NumericVector y, const NumericVector groups) {
  
    int n = x.size();
 
    int nfolds = unique(groups).size();
 
    NumericVector lambdas(nfolds);
    NumericVector mse(nfolds);
 
    for (int i =0; i < nfolds; i++){
        x_train = x[groups != (i+1)];
        y_train = y[groups != (i+1)];
    
        x_test = x[groups == (i+1)];
        y_test = y[groups == (i+1)];
     
    
    class Mse : public Functor {
      public:
      double operator()(const arma::vec& log_lambda) override {
        double lambda = exp(log_lambda[0]);
        return  mse_lambda(lambda, x_train, y_train, x_test, y_test);
      }
    };
    
    Mse fun;
    Roptim<Mse> opt("BFGS");
    
    arma::vec initial = {0.02};
    opt.minimize(fun, initial);
    
    
    arma::vec par = opt.par();
    lambdas[i] = exp(par[0]);
    mse[i] = opt.value();
    
  }
  
  int min_ind = which_min(mse);
  
  return lambdas[min_ind];
  
}



```
We now verify that this is consistent with our R result
```{r}
Rcpp_hat_lambda <- lambda_cv_Rcpp(x, y, groups)
abs(hat_lambda - Rcpp_hat_lambda)
```
We can now compare the computational times:
```{r}
microbenchmark("R" = lambda_cv(x, y, groups),
               "Rcpp" = lambda_cv_Rcpp(x, y, groups))
```
The results above show the function has been sped up by a factor of 2.

## Lambda as a function of x

We can see merely from looking at the plot, the shape of the function changes at approximately $x = 0.5$, and so these two sections of the function will need different values of $\lambda$. We address this by modelling $\lambda = \lambda(x)$. We do this by fitting the model as before for a fixed $\lambda$ (for this we can use the cross-validated value of $\lambda$) and consider the residuals $r_1, \ldots, r_n$.

We can then model these under another KRS with the same $\lambda$ - producing estimates of the absolute values of the residuals $\hat v_1, \ldots, \hat v_n$. We can then fit
$$
\hat \mu(x) = \frac{\sum_{i =1}^n \kappa_{\lambda_i} (x, x_i) \ y_i }{\sum_{i =1}^n \kappa_{\lambda_i} (x, x_i) }
$$
with $\lambda_i = \lambda\tilde w_i$ where $\tilde w_i = \frac{nw_i}{\sum_{i=1}^n w_i}$ for $w_i = \hat v_i^{-1}$.

We implement this in R with the `mean_var_KRS()` function:
```{r}
mean_var_KRS <- function(y, x, xnew, lambda){
  n <- length(x)
  nnew <- length(xnew)
  mu <- res <- numeric(n) 
   
  out <- madHat <- numeric(nnew)
 
  for(ii in 1:n){
    mu[ii] <- sum( dnorm(x, x[ii], lambda) * y ) / sum( dnorm(x, x[ii], lambda) )   
  }
 
  resAbs <- abs(y - mu)
  for(ii in 1:nnew){
    madHat[ii] <- sum( dnorm(x, xnew[ii], lambda) * resAbs ) / sum( dnorm(x, xnew[ii], lambda) )   
  }
 
  w <- 1 / madHat
  w <- w / mean(w)
 
  for(ii in 1:nnew){
    out[ii] <- sum( dnorm(x, xnew[ii], lambda * w[ii]) * y ) / 
             sum( dnorm(x, xnew[ii], lambda * w[ii]) )   
  }
 
  return(out)
}
```

and we use our cross-validated value of lambda to get:
```{r}
muSmoothAdapt <- mean_var_KRS( y, x, xnew, hat_lambda)
adapt_data <- tibble(xnew = xnew, fitted = muSmoothAdapt)
ggplot() +
  geom_point(data = data, aes(x, y), size = 0.8) +
  geom_line(data = adapt_data, aes(x = xnew,y = fitted), 
            color = "mediumorchid3", linewidth = 0.7)
```


We now write the complementary `Rcpp` version:
```{r, engine='Rcpp'}
#include <Rcpp.h>
#include <Rmath.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericVector mean_var_KRS_Rcpp(const NumericVector y, const NumericVector x, const NumericVector xnew, const double lambda){
  int n = x.size();
  int nnew = xnew.size();
  
  NumericVector mu(n);
  NumericVector res(n);
  NumericVector out(nnew);
  NumericVector madHat(nnew);
  
  for (int i = 0; i < n; i++){
    mu[i] = sum(dnorm(x,x[i], lambda)*y)/ sum(dnorm(x, x[i], lambda));
  }
  
  NumericVector resAbs = abs(y-mu);
  
  for (int i =0; i < nnew; i++){
    madHat[i] = sum(dnorm(x,xnew[i], lambda)*resAbs)/sum(dnorm(x,xnew[i], lambda));
  }
  NumericVector w = 1/madHat;
  w = w/mean(w);
  
  for (int i =0; i <nnew; i++){
    out[i] = sum(dnorm(x, xnew[i], lambda * w[i])*y) / sum(dnorm(x, xnew[i], lambda *w[i]));
  }
  
  return out;
}

```

We now check these two functions produce the same values:

```{r}
muSmoothAdapt_Rcpp <- mean_var_KRS_Rcpp( y, x, xnew, hat_lambda)
max(abs(muSmoothAdapt - muSmoothAdapt_Rcpp))
```

Comparing the time gain from this we seethe Rcpp version has about half of the computing time:
```{r}
microbenchmark("R" = mean_var_KRS(x, y, xnew, lambda = hat_lambda),
               "Rcpp" = mean_var_KRS_Rcpp(x, y, xnew, lambda = hat_lambda))
```

