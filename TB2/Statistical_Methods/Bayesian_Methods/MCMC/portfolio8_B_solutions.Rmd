---
title: "Portfolio 9 Solutions"
author: "Rachel Wood"
date: "2023-05-03"
output:
  pdf_document: default
  html_document: default
header-includes: \DeclareMathOperator*{\argmax}{arg\,max}
---

```{r setup, include=FALSE}
library(ggplot2)
theme_set(theme_bw())
knitr::opts_chunk$set(echo = TRUE)
```

We load the data and scale it as in the last portfolio for ease of comparison:
```{r}
library(mlbench)
data("PimaIndiansDiabetes")
head(PimaIndiansDiabetes)
```

```{r}
data <- PimaIndiansDiabetes
y <- as.numeric((data$diabetes))-1
X <- scale(data[,1:8])
```


# Choosing Proposal Distributions
For this we modify the joint proposal of the Metropolis Hastings algorithm:

$$
Q (z, dz') = \mathcal{N}_{p+1} (z, c \mathbf{\Sigma}_n)
$$
for tuning parameter $c>0$ and
$$
\mu_n = \argmax_{(\alpha, \beta) \in \mathbb{R}^p} \log \pi (\alpha, \beta | y), \hspace{1cm} \mathbf{\Sigma}_n = -(\mathbf{H}_n(\mu_q))^{-1} 
$$


In this, we instead want a proposal $Q_i$ for each parameter for which we use the conditional distribution under the multivariate normal:
$$
Q_i (z_t^{(i)} | z', dz') = \mathcal{N}_1 \left(z_{t-1}^{(i)}, \tilde \Sigma_n^{(i,i)} - \tilde \Sigma_n^{(i)} \tilde \Sigma_n'^{-1} \left( \tilde \Sigma_n^{(i)}\right)^T \right)
$$
where $z' = (z_t^{1:(i-1)}, z_{t-1}^{(i+1):p})$ and $\tilde \Sigma_n = c \Sigma_n$ 
We get $\mu_n$ and $\Sigma_n$ as before:
```{r}
fit <- glm(y ~ X, family = "binomial")
z_0 <- fit$coefficients
Sigma_n <- summary(fit)$cov.scaled
```

and write the function to obtain generate proposals:
```{r}
proposal <- function(z, i, sigma){
  var <- sigma[i,i] - sigma[i,-i] %*% solve(sigma[-i,-i]) %*% sigma[-i,i]
  z_i <- rnorm(1, z[i], sqrt(var))
  return(z_i)
}
```

# The Metropolis-within-Gibbs Algorithm
For this we generate proposals of each element of $z$ and accept it with probability:

$$
\alpha = \frac{\pi \left(z_t^{1:(i-1)}, \tilde z_t^{i}, z_{t-1}^{(i+1):p}| y \right)Q_i\left(z_{t-1}^{(i)}| \tilde z_t^{(i)}\right)}{\pi \left(z_t^{1:(i-1)}, z_{t-1}^{i}, z_{t-1}^{(i+1):p} | y \right)Q_i\left(\tilde z_t^{(i)}| z_{t-1}^{(i)} \right)} =  \frac{\pi \left(z_t^{1:(i-1)}, \tilde z_t^{i}, z_{t-1}^{(i+1):p}| y \right)}{\pi \left(z_t^{1:(i-1)}, z_{t-1}^{i}, z_{t-1}^{(i+1):p} | y \right)}
$$
since all $Q_j$ are symmetric.

Thus we need a function to compute $\pi (z| y) = L_n(z) \pi(z) = L_n(z)$ where $\pi(z)$ is uniformly distributed on $\mathcal{Z}$:
```{r}
library(mvtnorm)
post_likelihood <- function(par, X, y){
  
  alpha <- par[1]
  beta <- par[2:9]
  p <- 1 - 1 / (1 + exp(alpha + beta%*%t(X)))
  l_likelihood <- sum(dbinom(y, size=1, prob=p, log=TRUE))

  return(l_likelihood)
}
```


We now implement the MwG algorithm
```{r}
logistic_MwG <- function(z_0, c, sigma_n, tmax, X, y){
  zs <- matrix(NA, nrow =tmax, ncol = length(z_0))
  z_current <- z_0
  z_current_ll <- post_likelihood(z_current, X,y)
  sigma_prime <- c * sigma_n 
  accepted <- numeric(length(z_0))
  
  
  for (t in 1:tmax){
    for (i in 1:9){
      z_new <- z_current
      z_new[i] <- proposal(z_current, i, sigma_prime)
      z_new_ll <- post_likelihood(z_new, X, y)
      alpha <- exp(z_new_ll - z_current_ll)
      if (runif(1) < min(1, alpha)){
        z_current[i] <- z_new[i]
        z_current_ll <- z_new_ll
        accepted[i] <- accepted[i] +1
      }
      zs[t,] <- z_current
    }
    
  }
  return(list(zs = zs, acceptance_rate = accepted/tmax))
}
```

# Convergence

We now run the function on our data and obtain the initial acceptance rates for each variable:
```{r}
mwg<- logistic_MwG(z_0, 1, Sigma_n, 10000, X, y)
mwg$acceptance_rate
```

We then produce trace plots and autocorrelation plots:
```{r, message = FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)

colnames(mwg$zs) <- c("intercept", colnames(data)[1:8])
zs_plot <- mwg$zs %>%
  as_tibble() %>% 
  mutate(iteration = 1:10000) %>%
  pivot_longer(cols = 1:9)

ggplot(zs_plot, aes(iteration, value)) +
  geom_line(colour = "steelblue3") +
  facet_wrap(vars(name))

```
```{r}
par(mfrow = c(3,3))
for (i in 1:9){
  acf(mwg$zs[,i], main = colnames(data)[i])
}
```
The trace plots seem to show good mixing and the acf decreases quickly but the acceptance rate in very high.

# Modifying the proposal distributions
We now try a larger $c$ in the proposal distribution to lower the acceptance rate:
```{r}
mwg <- logistic_MwG(z_0, 10, Sigma_n, 10000, X, y)
mwg$acceptance_rate
```
This is much better, but we are looking for an acceptance rate of approximately 0.234 so we increase $c$ again to get:
```{r}
mwg <- logistic_MwG(z_0, 20, Sigma_n, 10000, X, y)
mwg$acceptance_rate
```
This is much closer to what we are looking for and we now check the same plots again:
```{r}
colnames(mwg$zs) <- c("intercept", colnames(data)[1:8])
zs_plot <- mwg$zs %>%
  as_tibble() %>% 
  mutate(iteration = 1:10000) %>%
  pivot_longer(cols = 1:9)

ggplot(zs_plot, aes(iteration, value)) +
  geom_line(colour = "steelblue3") +
  facet_wrap(vars(name))

```

```{r}
par(mfrow = c(3,3))
for (i in 1:9){
  acf(mwg$zs[,i], main = colnames(data)[i])
}
```
These all seem reasonable and so we move form
# Marginal Posterior Distribution
```{r}
ggplot(zs_plot, aes(value)) +
  geom_histogram() +
  facet_wrap(vars(name))
```


# Comparing with the Metropolis Hastings Results
Comparing the acf plots, we can see the Metropolis within Gibbs has much lower correlation between samples and seems to have better mixing:
```{r, include = FALSE}
logistic_MH <- function(z_0,c, sigma_n, tmax, X, y){
  zs <- matrix(NA, nrow =tmax, ncol = length(z_0))
  z_current <- z_0
  z_current_ll <- post_likelihood(z_current, X,y)
  sigma_prime <- c * sigma_n 
  accepted <- 0
  for (i in 1:tmax){
    z_new <- rmvnorm(1, z_current, sigma_prime)
    z_new_ll <- post_likelihood(z_new, X, y)
    alpha <- z_new_ll- z_current_ll
    if (runif(1) < min(1,exp(alpha))){
      
      z_current <- z_new
      z_current_ll <- z_new_ll
      accepted <- accepted + 1
      
    }
    zs[i,] <- z_current
  }
  return(list(zs = zs, acceptance_rate = accepted/tmax))
}
```

```{r, message = FALSE, warning = FALSE}
library(gridExtra)
mh<- logistic_MH(z_0, 0.75, Sigma_n, 10000, X, y)

colnames(mh$zs) <- c("intercept", colnames(data)[1:8])
mh_zs_plot <- mh$zs %>%
  as_tibble() %>% 
  mutate(iteration = 1:10000) %>%
  pivot_longer(cols = 1:9)

mh_mix <- ggplot(mh_zs_plot, aes(iteration, value)) +
  geom_line(colour = "steelblue3") +
  facet_wrap(vars(name)) +
  labs(title = "Metropolis Hastings")

mwg_mix <- ggplot(zs_plot, aes(iteration, value)) +
  geom_line(colour = "steelblue3") +
  facet_wrap(vars(name)) +
  labs(title = "Metropolis-within-Gibbs")

grid.arrange(mh_mix, mwg_mix, ncol = 2)
```
```{r}
par(mfrow = c(3,3))
for (i in 1:9){
  acf(mh$zs[,i], main = colnames(data)[i])
}
par(mfrow = c(3,3))
for (i in 1:9){
  acf(mwg$zs[,i], main = colnames(data)[i])
}
```
From these we conclude that the Metropolis-within-Gibbs sampler works better for the data.
