---
title: "Ridge, LASSO and Smoothing"
output: pdf_document
date: "2023-03-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(1234)
library(ggplot2)
theme_set(theme_bw())
```

# Task 1
For this task we use the `mogavs` dataset on communities and crime:

```{r}
library(mogavs)
data("crimeData")
```

We normalise and divide our data into training and testing sets:
```{r}
crimeData <- scale(crimeData)

train <- sample(1:nrow(crimeData), 1500)
X_train <- crimeData[train, -123]
y_train <- crimeData[train, 123]

X_test <- crimeData[-train, -123]
y_test <- crimeData[-train, 123]
```

## Plotting the Paths
We use the `glmnet` package to perform LASSO, setting the `alpha` parameter to be 1 and ridge regression, by using `alpha = 0`. This gives us the following path visualisations, with the left plot showing the LASSO regression and the right plot showing the ridge regression:
```{r}
library(glmnet)

lasso_fit <- glmnet(X_train, y_train, family = "gaussian", alpha = 1)


ridge_fit <- glmnet(X_train, y_train, family = "gaussian", alpha = 0)


par(mfrow = c(1,2))
plot(lasso_fit, xvar = "lambda")
plot(ridge_fit, xvar = "lambda")
```
We can see the coefficients in the ridge regression all steadily decrease to 0, where with the LASSO regression, the variable paths are more `wiggly' and will increase when other variables are set to 0.


## Cross Validation
We use the `cv.glmnet` function to perform cross validation on $\lambda$. We set the measure to be `"mse"` and stay with the default number of folds which is 10. We then produce plots of the $\log (\lambda)$ values, again with the LASSO on the left and ridge regression on the right:
```{r}
lasso_cv <- cv.glmnet(X_train, y_train, type.measure = "mse", alpha =1)
ridge_cv <- cv.glmnet(X_train, y_train, type.measure = "mse", alpha =0)

par(mfrow = c(1,2))
plot(lasso_cv)
plot(ridge_cv)

```
Here the lines show the $\log(\lambda)$ which minimises the MSE and the largest value of $\log (\lambda)$ such that the corresponding MSE is within one standard error of the minimum. 

For this we choose the $\lambda$s producing the minimum MSE, which are:
```{r}
print(paste("LASSO:", lasso_cv$lambda.min))
print(paste("Ridge:", ridge_cv$lambda.min))
```

## Comparison of results 
We first compare the coefficients resulting from our cross-validations:
```{r}
summary(coef(lasso_cv))
summary(coef(ridge_cv))
```
We can see that there are only 9 non-zero entries in our LASSO model, but all the variables are included in the ridge model. 


We now use the out-of-sample error obtained from the `predict()` function to compare the ridge regression and LASSO regression we obtained. 

```{r}
lasso_residuals <- y_test - predict(lasso_cv, X_test)
ridge_residuals <- y_test - predict(ridge_cv, X_test)

c(sum(lasso_residuals^2), sum(ridge_residuals^2))
```
We can see the LASSO model has the lower out-of sample prediction error in this case.


# Task 2
For this task we use the Bone Mineral Density dataset obtained from \url{https://hastie.su.domains/ElemStatLearn/}:
```{r}
data <- read.csv("spnbmd.csv", sep = "\t")
head(data)
```
For this we consider the model 
\begin{equation}
  Y_{i}^0 = f(x_i^0) + \varepsilon_i \text{ where } f \in \mathcal{C}^2 (\mathbb{R})
\end{equation}
where $Y_i^0$ corresponds the the `spnbmd` variable and $x_i^0$ corresponds to the `age` column. Further we fit seperate `f` functions for male and female entries.

We first separate the data by gender
```{r}
data_male <- data[data$gender == "male",]
data_female <- data[data$gender == "female",]
```

We then use the `gam()` function to model `spnbmd` with a cubic spline on `age` for both `data_male` and `data_female`. This uses general cross-validation to choose model parameters by default. 
```{r}
library(mgcv)
fit_male <- gam(spnbmd ~ s(age, bs = "cr", k =12), data = data_male)
fit_female <- gam(spnbmd ~ s(age, bs = "cc"), data = data_female)
```

We can plot the estimated functions and data point, colour-coded by gender:

```{r}
library(dplyr)
library(ggplot2)
data_female <- data_female %>%
  as_tibble() %>%
  mutate(fitted = fit_female$fitted.values)

data_male <- data_male %>%
  as_tibble() %>%
  mutate(fitted = fit_male$fitted.values)

data <- rbind(data_female, data_male) 

ggplot(data = data, aes(age, spnbmd, color = gender)) +
  geom_point(size = 1) +
  geom_line(aes(y = fitted))


``` 
